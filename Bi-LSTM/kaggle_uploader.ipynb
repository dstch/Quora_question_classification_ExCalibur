{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import csv, string, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters config\n",
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string(\"raw_train_data_path\", \"../raw_data/train.csv\", \"raw train data path\")\n",
    "flags.DEFINE_string(\"raw_test_data_path\", \"../raw_data/test.csv\", \"raw test data path\")\n",
    "flags.DEFINE_string(\"deal_train_data_path\", \"../train_data/deal_train_data.csv\", \"dealed train data path\")\n",
    "flags.DEFINE_string(\"train_data_path\", \"../train_data/train.csv\", \"train data path\")\n",
    "flags.DEFINE_string(\"dev_data_path\", \"../train_data/dev.csv\", \"dev data path\")\n",
    "flags.DEFINE_string(\"train_tfrecord_path\", \"../train_data/train_word_id.tf_record\", \"train data path\")\n",
    "flags.DEFINE_string(\"dev_tfrecord_path\", \"../train_data/dev_word_id.tf_record\", \"dev data path\")\n",
    "flags.DEFINE_integer(\"n_hidden\", 128, \"LSTM hidden layer num of features\")\n",
    "flags.DEFINE_integer(\"num_step\", 16, \"input data timesteps\")\n",
    "flags.DEFINE_integer(\"n_classes\", 2, \"number of classes\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.01, \"learnning rate\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"batch size\")\n",
    "flags.DEFINE_integer(\"max_steps\", 4000, \"max step,stop condition\")\n",
    "flags.DEFINE_integer(\"display_step\", 1000, \"save model steps\")\n",
    "flags.DEFINE_string(\"train_writer_path\", \"./logs/train\", \"train tensorboard save path\")\n",
    "flags.DEFINE_string(\"dev_writer_path\", \"./logs/train\", \"dev tensorboard save path\")\n",
    "flags.DEFINE_string(\"checkpoint_path\", \"./logs/checkpoint\", \"model save path\")\n",
    "flags.DEFINE_string(\"glove_path\", \"./glove.840B.300d/glove.840B.300d.txt\", \"pre-train embedding model path\")\n",
    "flags.DEFINE_string(\"gensim_path\", \"./glove.840B.300d/glove_model.txt\", \"pre-train embedding model path for gensim\")\n",
    "flags.DEFINE_string(\"vocab_path\", \"../train_data/vocab.txt\", \"pre-train embedding model path\")\n",
    "flags.DEFINE_integer(\"embedding_dim\", 300, \"word embedding dim\")\n",
    "flags.DEFINE_integer(\"seq_length\", 15, \"sentence max length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "def _read_csv(input_file):\n",
    "    \"\"\"\n",
    "    read csv file,get data\n",
    "    :param input_file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "        return lines[1:]  # remove header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-build data file\n",
    "train_data=pd.read_csv(FLAGS.raw_train_data_path)\n",
    "target_0_data=train_data.loc[train_data.target==0,:]\n",
    "target_1_data=train_data.loc[train_data.target==1,:]\n",
    "# view different target count\n",
    "print(\"target=0:%s\" % len(target_0_data),\"target=1:%s\" % len(target_1_data))\n",
    "# 打乱数据集\n",
    "target_0_data=target_0_data.sample(frac=1.0)\n",
    "target_1_data=target_1_data.sample(frac=1.0)\n",
    "# 切分数据集\n",
    "target_0_train,target_0_test=target_0_data.iloc[:80000],target_0_data.iloc[80000:]\n",
    "target_1_train,target_1_test=target_1_data.iloc[:80000],target_1_data.iloc[80000:]\n",
    "# 合并训练数据并保存\n",
    "deal_train_data=target_0_train.append(target_1_train)\n",
    "deal_train_data=deal_train_data.sample(frac=1.0)\n",
    "deal_train_data.to_csv(FLAGS.deal_train_data_path,index=False)\n",
    "# build train data\n",
    "random_all_train_data=deal_train_data.sample(frac=1.0)\n",
    "# 13w for train and 3w for dev\n",
    "train_data,dev_data=random_all_train_data.iloc[:130000],random_all_train_data.iloc[130000:]\n",
    "train_data.to_csv(FLAGS.train_data_path,index=False)\n",
    "dev_data.to_csv(FLAGS.dev_data_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change glove to gensim model\n",
    "def build_embedding_model(glove_file, gensim_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        num_lines = 0\n",
    "        for line in f:\n",
    "            num_lines += 1\n",
    "    dims = 300\n",
    "    gensim_first_line = \"{} {}\".format(num_lines, dims)\n",
    "    with open(glove_file, 'r', encoding='utf-8') as fin:\n",
    "        with open(gensim_file, 'w', encoding='utf-8') as fout:\n",
    "            fout.write(gensim_first_line + '\\n')\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "\n",
    "def sentence_split(sentence, max_length):\n",
    "    \"\"\"\n",
    "    remove punctuation and split sentence.return list of words\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——()?【】“”！，。？、~@#￥%……&*（）]+'\", \"\", sentence)\n",
    "    sentence = [x for x in sentence if x not in string.punctuation]\n",
    "    sentence = ''.join(sentence)\n",
    "    words = sentence.split()\n",
    "    if max_length == 0:\n",
    "        return words\n",
    "    else:\n",
    "        if len(words) > max_length:\n",
    "            words = words[:max_length]\n",
    "        elif len(words) < max_length:\n",
    "            words = words + [\" \"] * (max_length - len(words))\n",
    "        return words\n",
    "\n",
    "# build vocab\n",
    "def build_vocab(model_file, data_file, vocab_path):\n",
    "    # load glove model\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(model_file)\n",
    "    lines = _read_csv(data_file)\n",
    "    vocab = []\n",
    "    for line in lines:\n",
    "        vocab.extend(sentence_split(line[1], 0))\n",
    "    vocab = set(vocab)\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        for word in vocab:\n",
    "            if word in model:\n",
    "                f.write(word + ' ' + ' '.join([str(x) for x in model[word]]) + '\\n')\n",
    "             \n",
    "\n",
    "build_embedding_model(FLAGS.glove_path, FLAGS.gensim_file)\n",
    "build_vocab(FLAGS.gensim_file, FLAGS.deal_train_data_path, FLAGS.vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to tf-record\n",
    "def save_word_ids(save_path, csv_path, glove_path, embedding_dim, seq_length, mode='train'):\n",
    "    vocab, embd = loadGloVe(glove_path, embedding_dim)\n",
    "    # init vocab processor\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(seq_length)\n",
    "    # fit the vocab from glove\n",
    "    pretrain = vocab_processor.fit(vocab)\n",
    "    lines = _read_csv(csv_path)\n",
    "    split_lines = []\n",
    "    label_list = []\n",
    "    qid_list = []\n",
    "    if mode == 'test':\n",
    "        for line in lines:\n",
    "            split_lines.append(' '.join(sentence_split(line[1], seq_length)))\n",
    "            qid_list.append(str.encode(line[0]))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            split_lines.append(' '.join(sentence_split(line[1], seq_length)))\n",
    "            label_list.append(int(line[2]))\n",
    "            qid_list.append(str.encode(line[0]))\n",
    "    word_ids = list(vocab_processor.transform(np.array(split_lines)))\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(save_path)\n",
    "\n",
    "    if mode == 'test':\n",
    "        for index, line in enumerate(word_ids):\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"qid\":\n",
    "                    tf.train.Feature(bytes_list=tf.train.BytesList(value=[qid_list[index]])),\n",
    "                \"features\":\n",
    "                    tf.train.Feature(int64_list=tf.train.Int64List(value=line))\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "    else:\n",
    "        for index, line in enumerate(word_ids):\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"qid\":\n",
    "                    tf.train.Feature(bytes_list=tf.train.BytesList(value=[qid_list[index]])),\n",
    "                \"label\":\n",
    "                    tf.train.Feature(int64_list=tf.train.Int64List(value=[label_list[index]])),\n",
    "                \"features\":\n",
    "                    tf.train.Feature(int64_list=tf.train.Int64List(value=line))\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "\n",
    "save_word_ids(FLAGS.dev_tfrecord_path, FLAGS.dev_data_path, FLAGS.vocab_path, FLAGS.embedding_dim, FLAGS.seq_length)\n",
    "save_word_ids(FLAGS.train_tfrecord_path, FLAGS.train_data_path, FLAGS.vocab_path, FLAGS.embedding_dim, FLAGS.seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-lstm model\n",
    "class bi_lstm():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def model(self, n_hidden, input_data, weights, biases):\n",
    "        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "        lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.7)\n",
    "        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "        lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.7)\n",
    "\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, input_data, dtype=tf.float32)\n",
    "        # 双向LSTM，输出outputs为两个cell的output\n",
    "        # 将两个cell的outputs进行拼接\n",
    "        outputs = tf.concat(outputs, 2)\n",
    "        return tf.matmul(tf.transpose(outputs, [1, 0, 2])[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloVe(filename, emb_size):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    vocab.append('unk')  # 装载不认识的词\n",
    "    embd.append([0] * emb_size)  # 这个emb_size可能需要指定\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab, embd\n",
    "\n",
    "def build_embedding_layer(vocab, embd):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = len(embd[0])\n",
    "    embedding = np.asarray(embd)\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    return embedding_init, embedding, W, embedding_placeholder, vocab_size\n",
    "\n",
    "def read_from_tfrecords(tfrecord_dir, batch_size, max_length, n_classes, epochs, mode='train'):\n",
    "    \"\"\"\n",
    "    read data from tf_records\n",
    "    :param tfrecord_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # build file queue\n",
    "    file_queue = tf.train.string_input_producer([tfrecord_dir], num_epochs=epochs)\n",
    "    # build reader\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, value = reader.read(file_queue)\n",
    "    if mode == 'test':\n",
    "        features = tf.parse_single_example(value, features={\n",
    "            \"qid\": tf.FixedLenFeature([1], tf.string),\n",
    "            \"features\": tf.FixedLenFeature([max_length], tf.int64)\n",
    "        })\n",
    "        qid =features[\"qid\"]\n",
    "        vector = features[\"features\"]\n",
    "        vector_batch, qid_batch = tf.train.batch([vector, qid], batch_size=batch_size, num_threads=4, capacity=256)\n",
    "        return vector_batch, qid_batch\n",
    "    else:\n",
    "        features = tf.parse_single_example(value, features={\n",
    "            \"qid\": tf.FixedLenFeature([1], tf.string),\n",
    "            \"features\": tf.FixedLenFeature([max_length], tf.int64),\n",
    "            \"label\": tf.FixedLenFeature([1], tf.int64)\n",
    "        })\n",
    "\n",
    "        label = tf.cast(features[\"label\"], tf.int32)  # tf.cast(features[\"label\"], tf.string)\n",
    "        vector = features[\"features\"]\n",
    "\n",
    "        vector_batch, label_batch = tf.train.batch([vector, label], batch_size=batch_size, num_threads=4, capacity=256)\n",
    "\n",
    "        # deal with label batch, change int label to one-hot code\n",
    "        indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "        concated = tf.concat([indices, label_batch], 1)\n",
    "        onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, n_classes]), 1.0, 0.0)\n",
    "\n",
    "        return vector_batch, onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# tensorflow graph input\n",
    "input_data = tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.seq_length, FLAGS.embedding_dim], name='input_data')\n",
    "\n",
    "y = tf.placeholder(\"float\", [None, FLAGS.n_classes])\n",
    "\n",
    "# get data batch\n",
    "x_train_batch, y_train_batch = read_from_tfrecords(FLAGS.train_tfrecord_path, FLAGS.batch_size, FLAGS.seq_length,\n",
    "                                                   FLAGS.n_classes, 2)\n",
    "x_test, y_test = read_from_tfrecords(FLAGS.dev_tfrecord_path, FLAGS.batch_size, FLAGS.seq_length,\n",
    "                                     FLAGS.n_classes, 2)\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2 * FLAGS.n_hidden, FLAGS.n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([FLAGS.n_classes]))\n",
    "}\n",
    "\n",
    "pred = bi_lstm().model(FLAGS.n_hidden, input_data, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(pred), 1)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# tensorboard\n",
    "tf.summary.scalar('loss', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.group(tf.initialize_all_variables(), tf.local_variables_initializer())  # tf.global_variables_initializer()\n",
    "\n",
    "tf.logging.info('Start Training...')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.train_writer_path, sess.graph)\n",
    "    dev_writer = tf.summary.FileWriter(FLAGS.dev_writer_path, sess.graph)\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    step = 1\n",
    "    tf.logging.info(\"into embedding layer\")\n",
    "    # embedding layer\n",
    "    vocab, embd = loadGloVe(FLAGS.glove_path, FLAGS.embedding_dim)\n",
    "    embedding_init, embedding, W, embedding_placeholder, vocab_size = build_embedding_layer(vocab, embd)\n",
    "    W = sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "    # embedding text\n",
    "    x_train_batch = tf.nn.embedding_lookup(W, x_train_batch, name='train_text_embedding')\n",
    "    x_test = tf.nn.embedding_lookup(W, x_test, name='test_text_embedding')\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            curr_x_train_batch, curr_y_train_batch = sess.run([x_train_batch, y_train_batch])\n",
    "            # tf.logging.info(\"start %s step optimizer\" % step)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                input_data: curr_x_train_batch,\n",
    "                y: curr_y_train_batch\n",
    "            })\n",
    "            if step % FLAGS.every_step == 0:\n",
    "                # Calculate batch accuracy and loss\n",
    "                acc, loss, summary = sess.run([accuracy, cost, merged_summary],\n",
    "                                              feed_dict={input_data: curr_x_train_batch, y: curr_y_train_batch})\n",
    "\n",
    "                tf.logging.info(\"Iter \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                                \"{:.5f}\".format(acc))\n",
    "                train_writer.add_summary(summary, step)\n",
    "            if step % FLAGS.display_step == 0 or coord.should_stop():\n",
    "                curr_x_test_batch, curr_y_test_batch = sess.run([x_test, y_test])  # shape(32,15)\n",
    "                # Calculate test batch accuracy and prediction value\n",
    "                test_acc, pre, test_summary = sess.run([accuracy, y_pred_cls, merged_summary],\n",
    "                                                       feed_dict={input_data: curr_x_test_batch, y: curr_y_test_batch})\n",
    "                print(\"Step:%s ,Testing Accuracy:\" % step, test_acc)\n",
    "                # save model\n",
    "                saver.save(sess, FLAGS.checkpoint_path + '/model-%s' % step, global_step=step)\n",
    "                # get real value\n",
    "                y_true = curr_y_test_batch[:, 1]\n",
    "                # calculate evaluate value\n",
    "                # tf_p, tf_r, tf_f1 = sess.run(calculate_evaluate_value(pre, y_true))\n",
    "                # print(\"prediction:%s   recall:%s   f1_score:%s\" % (tf_p, tf_r, tf_f1))\n",
    "                # \n",
    "                # # evaluate by sklearn\n",
    "                # # 评估\n",
    "                # print(\"Precision, Recall and F1-Score...\")\n",
    "                # print(metrics.classification_report(y_true, pre, target_names=['无意义', '有意义']))\n",
    "                # \n",
    "                # # 混淆矩阵\n",
    "                # print(\"Confusion Matrix...\")\n",
    "                # cm = metrics.confusion_matrix(y_true, pre)\n",
    "                # print(cm)\n",
    "\n",
    "                with tf.name_scope('Evaluation'):\n",
    "                    tf.summary.scalar('prediction', tf_p)\n",
    "                    tf.summary.scalar('recall', tf_r)\n",
    "                    tf.summary.scalar('f1_score', tf_f1)\n",
    "                dev_writer.add_summary(test_summary, step)\n",
    "\n",
    "            step += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        tf.logging.info(\"Optimization Finished!\")\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test data tf_record\n",
    "if os.path.exists(FLAGS.test_tfrecord_path) is not True:\n",
    "    save_word_ids(FLAGS.test_tfrecord_path, FLAGS.test_data_path, FLAGS.glove_path, FLAGS.embedding_dim,\n",
    "                  FLAGS.seq_length, 'test')\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # tensorflow graph input\n",
    "    input_data = tf.placeholder(dtype=tf.float32, shape=[None, FLAGS.seq_length, FLAGS.embedding_dim],\n",
    "                                name='input_data')\n",
    "    # get test data\n",
    "    test_text_batch, test_qid_batch = read_from_tfrecords(FLAGS.test_tfrecord_path, FLAGS.batch_size, FLAGS.seq_length,\n",
    "                                                          FLAGS.n_classes, 1, 'test')\n",
    "    # ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_path)  # 通过检查点文件锁定最新的模型\n",
    "    # saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path + '.meta')  # 载入图结构，保存在.meta文件中\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "        'out': tf.Variable(tf.random_normal([2 * FLAGS.n_hidden, FLAGS.n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([FLAGS.n_classes]))\n",
    "    }\n",
    "    pred = bi_lstm().model(FLAGS.n_hidden, input_data, weights, biases)\n",
    "    logit = tf.argmax(tf.nn.softmax(pred), 1)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # sess.run(tf.initialize_all_variables())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        # embedding layer\n",
    "        vocab, embd = loadGloVe(FLAGS.glove_path, FLAGS.embedding_dim)\n",
    "        embedding_init, embedding, W, embedding_placeholder, vocab_size = build_embedding_layer(vocab, embd)\n",
    "        W = sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "        # embedding text\n",
    "        test_text_batch = tf.nn.embedding_lookup(W, test_text_batch, name='train_text_embedding')\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)  # 载入参数，参数保存在两个文件中，不过restore会自己寻找\n",
    "        else:\n",
    "            tf.logging.ERROR(\"Load model failed, can't find model\")\n",
    "            raise FileNotFoundError\n",
    "        # graph = tf.get_default_graph()\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        save_qid_list = []\n",
    "        save_target_list = []\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                test_batch_value, qid = sess.run([test_text_batch, test_qid_batch])\n",
    "                pred_value = sess.run(logit, feed_dict={input_data: test_batch_value})\n",
    "                save_qid_list.extend(qid)\n",
    "                save_target_list.extend(pred_value)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Done testing -- epoch limit reached\")\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        # write result to csv\n",
    "        headers = ['qid', 'target']\n",
    "        save_data_list = []\n",
    "        for index, qid in enumerate(save_qid_list):\n",
    "            qid_str = qid[0].decode()\n",
    "            save_data_list.append([qid_str, save_target_list[index]])\n",
    "        with open(FLAGS.submit_file, 'w+') as f:\n",
    "            f_csv = csv.writer(f)\n",
    "            f_csv.writerow(headers)\n",
    "            f_csv.writerows(save_data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}